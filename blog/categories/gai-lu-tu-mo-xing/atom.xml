<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 概率图模型 | 玩儿数据]]></title>
  <link href="http://wangyinanchina.github.io/blog/categories/gai-lu-tu-mo-xing/atom.xml" rel="self"/>
  <link href="http://wangyinanchina.github.io/"/>
  <updated>2014-09-11T13:29:02+08:00</updated>
  <id>http://wangyinanchina.github.io/</id>
  <author>
    <name><![CDATA[王轶楠]]></name>
    <email><![CDATA[Yinan.wang@pku.edu.cn]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[概率图模型：第二章(概率论部分)-读书笔记]]></title>
    <link href="http://wangyinanchina.github.io/blog/2014/09/10/pgm-chap-2/"/>
    <updated>2014-09-10T18:33:13+08:00</updated>
    <id>http://wangyinanchina.github.io/blog/2014/09/10/pgm-chap-2</id>
    <content type="html"><![CDATA[<p>这学期开始阅读概率图模型的大部头：《Probabilistic Graphical Models Principles and Techniques》。本学期的阅读计划是贝叶斯网络相关的：2，3，4，5，17，18，19，21章，希望可以坚持下来~~</p>

<p>接下来进入正文，第二章主要是复习之前的概率论和图论的知识。<!--more--></p>

<h3>概率论部分</h3>

<p>人们通常用概率来度量一件事发生的可能性的大小，或者内心中对一件事发生的信度。如果用$$$\Omega$$$来表示一件事所有可能的结果，举例：</p>

<blockquote><p>如果我们仍一个骰子，那么$$$\Omega = \{1, 2, 3, 4, 5, 6\}$$$</p></blockquote>

<p>对于$$$\Omega$$$中的子集，如果该子集可以赋以概率，比如扔一次骰子得到奇数的概率为0.5，那么子集{1, 2, 3}就可以认为是可测的。$$$\Omega$$$中的可测子集组成S。根据以上概念，我们可以给出<em>事件空间</em>的定义：</p>

<blockquote><ul>
<li>事件空间包含空集 $$$\emptyset$$$和全空间 $$$\Omega$$$</li>
<li>事件空间对于交运算封闭，即如果 $$$\alpha, \beta \in S$$$，则 $$$\alpha \cup\beta\in S$$$</li>
<li>事件空间对于求余运算封闭，即如果$$$\alpha \in S$$$, 则$$$\Omega - \alpha \in S$$$</li>
</ul>
</blockquote>

<p>有了事件空间的概念之后，那么我们可以发现，所谓的概率就是把一个事件映射到一个实数上，那么，我们就可以比较严格的给出概率空间$$$(\Omega, S, P)$$$中P的定义：</p>

<blockquote><p>概率P需要满足以下三个条件：</p>

<ul>
<li>$$$P(\alpha) \geqq0,对任意\alpha \in S$$$</li>
<li>$$$P(\Omega) = 1$$$</li>
<li>如果$$$\alpha, \beta \in S 以及\alpha \cap \beta = \emptyset，那么P(\alpha\cup\beta) = P(\alpha) + P(\beta)$$$</li>
</ul>
</blockquote>

<p>现在讨论条件概率。如果有两个事件$$$\alpha和\beta$$$，已知事件$$$\alpha$$$已经发生，那么$$$\beta$$$发生的概率怎么估计？我们定义：
$$P(\beta|\alpha) = P(\alpha \cap \beta) / P(\alpha)$$</p>

<p>这个公式可以简单理解为事件$$$\alpha和\beta$$$同时发生的概率分解为事件$$$\alpha$$$发生，然后在$$$\alpha$$$发生的情况下，$$$\beta$$$发生的概率。以上公式可以推广到<strong>链式法则</strong>
$$P(\alpha_{1}\cap&hellip;\cap\alpha_{k}) = P(\alpha_{1})P(\alpha_{2}|\alpha_{1})&hellip;P(\alpha_{k}|\alpha_{1}\cap&hellip;\cap\alpha_{k -1})$$
以及<strong>贝叶斯</strong>公式：
$$P(\alpha|\beta) = P(\beta|\alpha)P(\alpha) / P(\beta)$$</p>

<p>讨论完事件之后，我们试图讨论更抽象的概念：<strong>随机变量</strong>。随机变量在本质上是一个可测函数，举例：</p>

<blockquote><p>设随机变量Grade，代表学生在一门课中的成绩。设学生的成绩有A，B，C三种，全体学生组成全集$$$\Omega$$$，则Grade是从$$$\Omega$$$到{A, B, C}的一个映射。Grade = A即代表事件$$$\{\omega\in\Omega:f_{Grade}=A\}$$$。</p></blockquote>

<p>随机变量的取值可以是离散的或者连续的，所以随机变量分为离散型和连续型随机变量。对离散型随机变量X来说，设X的取值为$$$\{x_1, x_2,&hellip;,x_k\}$$$，那么，$$$\sum_{i = 1}^{k}P(X = x_i) = 1$$$。</p>

<p>如果我们同时考虑两个随机变量的概率分布，那么就引入了<strong>联合分布</strong>和<strong>边缘分布</strong>的概念。如果我们需要同时考虑几个随机变量的分布，那么这些随机变量遵从一个联合分布。而其中一个随机变量的边缘分布是其余随机变量取尽其可能值的情况下的分布。</p>

<p>接下来我们讨论<strong>条件概率</strong>，条件概率是指我们同时考虑几个随机变量，这些随机变量遵从一个联合分布。如果我们知道其中一个随机变量的取值并将其固定，那么其余随机变量的概率分布也会发生改变。举例：</p>

<blockquote><p>我们如果需要考虑两个随机变量，一个是学生的智力，另一个是学生的考试成绩。那么，当我们知道学生的考试成绩为A的时候，我们有更大的把握认为该学生的智力高。</p></blockquote>

<p>如果我们知道另一个随机变量的信息之后，其余随机变量的分布不改变，那么我们就说这个随机变量和其余的独立。事件之间也可以有独立性，例如：</p>

<blockquote><p>对于随机变量：掷硬币的结果，每一次的结果都是一个事件，而每次结果之间独立，因为每一次的结果都和前面的结果无关。这个是事件之间的独立性。
而对于两个硬币A和B，掷硬币A的结果和掷硬币B的结果独立，这是随机变量之间的独立性。</p></blockquote>

<p>但是在现实中，找到独立的随机变量比较困难。更一般的情况是<strong>条件独立</strong>。</p>

<blockquote><p>假设我们有随机变量X，Y，Z，那么X和Y条件独立是指当给定Z的时候，X和Y独立。即$$$P(X,Y|Z) = P(X|Z)P(Y|Z)$$$</p></blockquote>

<p>在现实生活中，我们通常会遇到的情况是，我们知道一些信息，而希望推测另一部分信息，比如：</p>

<blockquote><p>我们知道当天的风速，气温，云层厚度等，我们希望知道明天会不会下雨。</p></blockquote>

<p>这其实就是条件概率问题，我们将风速、气温和云层厚度这三个随机变量取值后，我们希望得到明天下雨的概率，即得到下雨的后验概率分布。通常这种问题称为概率索引。</p>

<p>另一种问题是，当我们知道已有的信息之后，我们想知道最可能发生的是什么，比如我们知道风速、气温以及云层厚度之后，我们想知道第二天的天气最有可能是什么，这是一个MAP索引。</p>

<p>需要注意的是，当我们感兴趣的变量大于一个的时候，MAP索引和概率索引的结果是很不一样的。我们不能单纯的计算将我们感兴趣的变量赋予给定条件下条件概率最大的那个结果，然后认为MAP索引的结果就是上述结果的总成。我的理解是，概率索引是计算条件概率，MAP索引是在联合分布上寻找极大值。</p>

<p>之前我们的讨论着重于离散变量。对于连续变量X，P(X = x)总是等于0，所以我们引进<strong>概率密度函数（PDF）</strong>来定义连续变量的概率分布。</p>

<blockquote><p>概率密度函数是非负可积函数，对于连续型随机变量X，概率密度函数p在X的所有可能取值Val(x)上的积分为1。即$$$\int_{Val(X)}p(x)dx = 1$$$。而对于某个区间的概率，$$$P(a \leq X \leq b) = \int_{a}^{b}p(x)d(x)$$$。</p></blockquote>

<p>最简单的概率密度函数是<strong>均匀分布</strong>。</p>

<blockquote><p>我们称变量X在[a, b]上遵从均匀分布，记为X~Unif[a, b]，如果X有PDF：
$$p(x) =  1/(b-a), a\leq x \leq b; 0, otherwise$$</p></blockquote>

<p>另一种常见分布是<strong>高斯分布</strong>。</p>

<blockquote><p>我们说一个随机变量X服从均值为$$$\mu$$$，方差为$$$\sigma<sup>2</sup>$$$的高斯分布，记为$$$X~N(\mu;\sigma<sup>2</sup>)$$$，如果X的PDF为$$$p(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)<sup>2</sup>}{2\sigma<sup>2</sup>}}$$$。</p></blockquote>

<p>对于多个随机变量，他们的联合密度函数按照如下方式定义：</p>

<blockquote><p>设P使随机变量$$$X_{1},&hellip;,X_{n}$$$，则$$$p(x_{1},&hellip;,x_{n})$$$是$$$X_{1},&hellip;,X_{n}$$$的联合概率密度函数，当其满足:</p>

<ul>
<li>$$$p(x_{1},&hellip;,x_{n}) \geq 0$$$</li>
<li>p可积</li>
<li>对$$$a_{1},&hellip;,a_{n}$$$和$$$b_{1},&hellip;,b_{n}$$$，$$$P(a_{1}\leq X_{1} \leq b_{1},&hellip;,a_{n}\leq X_{n} \leq b_{n}) = \int_{a_1}^{b_{1}}&hellip;\int_{a_n}^{b_{n}}p(x_{1},&hellip;,x_{n})dx_{1}&hellip;d_{n}$$$。</li>
</ul>
</blockquote>

<p>对于连续变量的边缘分布，设随机变量X,Y，则$$$p(x) = \int_{-\infty}^{\infty}p(x,y)dy$$$</p>

<p>连续变量的条件密度类似于离散变量的条件概率，即$$p(x|y) = \frac{p(x)p(y|x)}{p(y)}$$
连续型变量间的条件独立定义为：</p>

<blockquote><p>设X,Y,Z是随机变量，有联合密度p(X,Y,Z)，我们说给定Z的情况下X和Y条件独立，当：$$$p(x|z) = p(x|y, z)$$$</p></blockquote>

<p>最后讨论期望和方差，这是对随机变量的两个描述性统计量，分别反映了随机变量的取值和波动。</p>

<p>对离散型随机变量X，期望为：$$E(X) = \sum_{x}x*P(x)$$</p>

<p>对连续型随机变量X，期望为：$$E(X) = \int x*p(x)dx$$</p>

<p>函数的期望具有线性，即$$E(aX + Y) = aE(X) + E(Y)$$
如果X和Y独立，那么$$E(X*Y) = E(X)E(Y)$$
对于X和Y的条件期望，有公式:$$E(X|y) = \sum_{x}x*P(x|y)$$</p>

<p>方差的定义如下：</p>

<blockquote><p>$$$Var(X) = E[(X - E(x))<sup>2</sup>] = E(X<sup>2</sup>) - (E(X))<sup>2</sup>$$$</p></blockquote>

<p>如果两个变量独立，则$$$Var(X+Y) = Var(X) + Var(Y)$$$，否则需要考虑协方差。</p>

<p>函数的方差：$$$Var(aX + b) = a<sup>2</sup> Var(X)$$$。</p>

<p>标准差定义为方差的平方根。</p>

<p>最后是切比雪夫不等式（我已经忘记怎么证明了&hellip;不过当年上课时肯定是会证的！！！）:</p>

<blockquote><p>$$P(|X-E(X)| \geq t) \leq Var(X) / t<sup>2</sup>$$</p></blockquote>

<p>它的一个重要用途是告诉我们观测值偏离期望的大致概率。</p>
]]></content>
  </entry>
  
</feed>
