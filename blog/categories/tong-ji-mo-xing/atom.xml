<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 统计模型 | 玩儿数据]]></title>
  <link href="http://wangyinanchina.github.io/blog/categories/tong-ji-mo-xing/atom.xml" rel="self"/>
  <link href="http://wangyinanchina.github.io/"/>
  <updated>2014-10-04T16:14:38+08:00</updated>
  <id>http://wangyinanchina.github.io/</id>
  <author>
    <name><![CDATA[王轶楠]]></name>
    <email><![CDATA[Yinan.wang@pku.edu.cn]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[统计模型：理论与实践 第五章：多元回归-特别主题 读书笔记]]></title>
    <link href="http://wangyinanchina.github.io/blog/2014/09/15/statistical-model-chap-5/"/>
    <updated>2014-09-15T21:28:48+08:00</updated>
    <id>http://wangyinanchina.github.io/blog/2014/09/15/statistical-model-chap-5</id>
    <content type="html"><![CDATA[<p>在多元回归模型：$$Y = X\beta + \epsilon\qquad(1)$$
中，<!--more-->我们通常假定：1、$$$\epsilon$$$独立同分布；2、$$$\epsilon有均值0和方差\sigma^{2}$$$。在这一章，我们要讨论的问题是，如果$$$\epsilon_{i}$$$之间不独立，而是服从假定：$$E(\epsilon|X) = 0_{n*1}, \quad cov(\epsilon|X) = \sigma^{2}I_{n*n} \qquad(2)$$
将对多元回归产生怎样的影响。</p>

<p>下面将不加证明的给出</p>

<blockquote><h4>Gauss-Markov定理:</h4>

<p>假定X是非随机的，即$$$E(\epsilon) = 0_{n*1}, \quad cov(\epsilon) = \sigma^{2}I_{n*n}$$$,那么OLS估计量，即$$$\widehat{\beta} = (X&#8217;X)^{-1}X&#8217;Y$$$估计量是BLUE（最好线性无偏估计量，此时$$$var(\widehat{\beta})$$$最小）。</p></blockquote>

<p>如果我们将约束条件变得更加严格一些，即$$E(\epsilon|X) = 0_{n*1},\quad cov(\epsilon|X) = G,\quad G是n*n的正定矩阵\qquad(3)$$
这就是一个GLS回归模型。此时，相对于一般的回归模型，有以下两个区别：</p>

<blockquote><ul>
<li>$$$\widehat{\beta}_{OLS}不再是BLUE（最好线性无偏估计量）$$$</li>
<li>$$$cov(\widehat{\beta}_{OLS}|X) = (X&#8217;X)^{-1}X&#8217;GX(X&#8217;X)^{-1}$$$</li>
</ul>
</blockquote>

<p>为了解决$$$\widehat{\beta}_{OLS}不再是BLUE（最好线性无偏估计量）$$$的问题，我们可以将(1)左乘$$$G^{-&frac12;}$$$，则此时方程满足(2)，那么我们可以得到：
$$\widehat{\beta}_{GLS} = (X&#8217;G^{-1}X)^{-1}X&#8217;G^{-1}Y\qquad(4)$$
可以证明$$$\widehat{\beta}_{GLS}$$$条件无偏以及在X固定的情况下，$$$\widehat{\beta}_{GLS}$$$是BLUE。此时，可以计算出：
$$cov(\widehat{\beta}_{GLS}|X) = cov((X&#8217;G^{-1}X)^{-1}X&#8217;G^{-1}(X\beta +\epsilon)|X) = (X&#8217;G^{-1}X)^{-1}X&#8217;G^{-1}GG^{-1}X(X&#8217;G^{-1}X)^{-1} = (X&#8217;G^{-1}X)^{-1}\qquad(5)$$</p>

<p>在通常情况下，G未知，需要通过数据来估计。假设G是一个n*n的矩阵，那么我们要估计n*n个参数，这样待估计的参数远远超过了n个数据点，所以我们需要对G做出相应的约束。通常可以做出两种约束：</p>

<ul>
<li>认为$$$G=\lambda\Gamma$$$，其中$$$\Gamma$$$是固定的正定矩阵，所以我们只需要估计$$$\lambda$$$，$$$\lambda$$$可以用均方残差来估计</li>
<li>认为G的对角线是2*2的正定矩阵K，这样我们只需要估计$$$K_{11},K_{12},K_{22}$$$。</li>
</ul>


<p><strong>综上可以发现，实际上$$$\widehat{\beta}_{OLS}和\widehat{\beta}_{GLS}在给定X的情况下都是无偏的，我们考虑\widehat{\beta}_{GLS}的主要原因是\widehat{\beta}_{OLS}在该情况下非BLUE$$$。</strong></p>

<p>假设X是n*p矩阵，有时候并不是所有p个变量都对结果有影响。所以我们需要检验对应于$$$X_{i}的\beta_{i}$$$是否显著。对于OLS模型，我们使用t分布来做这个检验。首先我们定义t分布：</p>

<blockquote><p>设随机变量$$$U_{1},U_{2},&hellip;为IID（独立同分布）N(0,1),则\sum_{i=1}^{d}U_{i}^{2}服从自由度为d的卡方分布\chi_{d}^{2}，则\frac{U_{d+1}}{\sqrt{d^{-1}\sum_{i=1}^{d}U_{i}^{2}}}服从自由度为d的t分布$$$</p></blockquote>

<p>接下来我们不加证明的给出一个定理：</p>

<blockquote><p>如果误差是IID $$$N(0, \sigma^{2})$$$分布，那么$$$\widehat{\beta}_{OLS}服从均值为\beta,协方差矩阵为\sigma^{2}(X&#8217;X)^{-1}$$$，而且$$$e和\widehat{\beta}_{OLS}独立，||e||^{2} 服从 \sigma^{2}\chi_{d}^{2}，其中d=n-p$$$</p></blockquote>

<p>根据上述定理，我们可以得到推论：</p>

<blockquote><p>对于零假设：$$$\beta_{k}=0, t=\widehat{\beta}_{k}/\widehat{SE}$$$服从自由度为n-p的t分布,其中$$$\widehat{SE}=\sqrt{\widehat{\sigma}*(X&#8217;X)^{-1}_{kk}}$$$</p></blockquote>

<p>这样的话，我们就可以用t分布来对零假设进行假设检验了。</p>

<p>如果要检验多个$$$\widehat{\beta}_{i}$$$是否为0，我们则选用F检验，检验的方法如下：</p>

<blockquote><p>设我们有OLS模型，如式（1），X满秩，为n*p矩阵。我们要检验<strong>零假设</strong>:p个$$$\widehat{\beta}_{i}中前p_{0}个不为0，从p_{0}+1起均为0$$$。我们需要首先计算（1）的$$$\widehat{\beta}$$$，再计算满足0假设的$$$\widehat{\beta}^{(s)}$$$，则检验统计量为：
$$F=\frac{(||X\widehat{\beta}||^{2}-||X\widehat{\beta}^{(s)}||^{2})/p_{0}}{||e||^{2}/(n-p)}$$
这里不加证明的给出一个结论：在零假设下，F服从Fisher F分布。</p></blockquote>

<p>这一章的最后一部分讲的是统计显著的结果也可能是不可靠的。这很容易想象，因为如果我们设定显著为0.05，那么即使零假设是对的，我们也有0.05的可能得到一个显著地结果，例如100次试验可以得到5次假阳性。除此之外，我们在做回归的时候，如果我们先用所有的变量进行回归，进而剔除掉为系数为零的变量，用剩下的变量来对模型进行回归，我们可能会得到一个比较好的$$$R^{2}$$$，但是这往往不够可靠。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[《统计模型：理论与实践》第四章：多元回归-读书笔记]]></title>
    <link href="http://wangyinanchina.github.io/blog/2014/09/03/tong-ji-mo-xing-duo-yuan-hui-gui/"/>
    <updated>2014-09-03T17:16:23+08:00</updated>
    <id>http://wangyinanchina.github.io/blog/2014/09/03/tong-ji-mo-xing-duo-yuan-hui-gui</id>
    <content type="html"><![CDATA[<p>多元回归和单变量回归类似，目的是探索自变量和因变量的关系，以及对因变量的取值给予预测。</p>

<!--more-->


<p>由于多元回归包含多个自变量，因此要复杂一些。通常情况下，我们假设自变量为矩阵$$$X$$$，因变量向量为$$$Y$$$，随机误差为$$$\epsilon$$$，那么，我们认为他们之间的关系是：$$Y = X\beta + \epsilon \qquad(1)$$
显然$$$X, Y, \beta以及\epsilon$$$都是随机变量，我们往往能够从数据中得到$$$X$$$和$$$Y$$$的一些观测值，但是仅有这些并不足以使我们推断出$$$\beta$$$，例如我们可以使$$$\beta$$$为任意值，只要调整$$$\epsilon$$$即可。所以，我们还需要一些假设来帮助我们推断$$$\beta$$$。通常我们假设：$$X与\epsilon独立，即X\perp\perp\epsilon \qquad(2)$$
以及$$\epsilon_i独立同分布, 期望为0，方差为\sigma^{2} \qquad(3)$$</p>

<p>以上两个假设对$$$\epsilon$$$的取值做了限制，使得我们有可能去估计$$$\beta$$$。通常我们选择OLS估计量$$$\widehat{\beta} = (X&#8217;X)^{-1}X&#8217;Y$$$来估计$$$\beta$$$，并且定义残差为:$$e = Y - X\widehat{\beta} \qquad(4)$$
可以证明，当<a href="http://wangyinanchina.github.io/blog/2014/09/03/prove-beta-ols/">$$$\beta = \widehat{\beta}时，sum(e^{2})最小$$$</a>,并且$$$e和X正交，即e\perp X$$$.
在这里，需要注意e和$$$\epsilon$$$的区别: $$$\epsilon$$$是随机误差，无法观测，也无法估计，我们只能假定它的分布、期望和方差。而e是可以观测的，当我们得到$$$\widehat{\beta}$$$之后，我们也就得到了e，并且通常会用e的平方和的大小去评价一个回归模型的好坏。除此之外，$$$\epsilon$$$与$$$X$$$ 独立而不一定正交，而$$$e与X$$$ 正交，但并不独立。</p>

<p>$$$\widehat{\beta}$$$的另一个重要性质是它是条件无偏的，即$$$E(\widehat{\beta}|X) = \beta$$$。证明过程如下：</p>

<blockquote><p>$$E(\widehat{\beta}|X) = E((X&#8217;X)^{-1}X&#8217;Y|X) = (X&#8217;X)^{-1}X&#8217;E(X\beta + \epsilon|X) = (X&#8217;X)^{-1}X&#8217;X\beta = \beta$$</p></blockquote>

<p>得到$$$\widehat{\beta}$$$后，我们自然要关注其误差是多少，这样我们可以对估计值的波动范围有一个了解。我们可以计算出，$$$cov(\widehat{\beta}|X) = \sigma^{2}(X&#8217;X)^{-1}$$$，计算过程如下:</p>

<blockquote><p>已知$$\widehat{\beta} = (X&#8217;X)^{-1}X&#8217;Y = (X&#8217;X)^{-1}X&#8217;(X\beta + \epsilon) = \beta + (X&#8217;X)^{-1}X&#8217;\epsilon\qquad(5)$$
所以，$$cov(\widehat{\beta}|X) = (X&#8217;X)^{-1}X&#8217;cov(\epsilon|X)X(X&#8217;X)^{-1} = \sigma^{2}(X&#8217;X)^{-1}\qquad(6)$$
原式得证。</p></blockquote>

<p>但是$$$\sigma^{2}$$$是无法观测的，所以我们必须得到$$$\widehat{\sigma^{2}}$$$，通常我们使用e&#8217;e来估计$$$\widehat{\sigma^{2}}$$$，由于e一般要比$$$\epsilon$$$要小（因为估计$$$\beta$$$的时候我们选择使e&#8217;e最小的$$$\beta$$$），所以我们将e&#8217;e除以自由度n - p来更好的估计$$$\widehat{\sigma^{2}}$$$，即：$$$\widehat{\sigma^{2}} = 1/(n - p) * e&#8217;e$$$。可以证明，$$$E(\widehat{\sigma}^2|X) = \sigma^{2}$$$，证明过程如下：</p>

<blockquote><p>已知$$E(\widehat{\sigma}^2|X) = 1/(n - p) * E(e&#8217;e|X)\qquad(7)$$，定义$$$H = X(X&#8217;X)^{-1}X&#8217;$$$，易见H是对称幂等矩阵，则$$e = Y - X\widehat{\beta} = (I - H)Y = (I - H)(X\beta + \epsilon) = (I -H)\epsilon\qquad(8)$$
所以$$e&#8217;e = \epsilon&#8217;(I - H)&lsquo;(I - H)\epsilon = \epsilon&rsquo;(I - H)\epsilon = \sum_{i = 1}^{n}\sum_{j = 1}^{n}\epsilon_{i}(I - H)_{ij}\epsilon_{j}\qquad(9)$$
所以$$E(\widehat{\sigma}^2|X) = E(\sum_{i = 1}^{n}\sum_{j = 1}^{n}\epsilon_{i}(I - H)_{ij}\epsilon_{j}|X) = \sum_{i = 1}^{n}\sum_{j = 1}^{n}(I - H)_{ij}E(\epsilon_{i}\epsilon_{j}|X)\qquad(10)$$
又因为$$$\epsilon$$$和X相互独立，且$$$\epsilon$$$独立同分布，所以$$E(\epsilon_{i}\epsilon_{i}|X) = \sigma^{2}, E(\epsilon_{i}\epsilon_{j}|X) = 0\qquad(11)$$
所以，$$\sum_{i = 1}^{n}\sum_{j = 1}^{n}(I - H)_{ij}E(\epsilon_{i}\epsilon_{j}|X) = \sigma^{2} trace(I -H) = \sigma^{2} (n - trace(X(X&#8217;X)^{-1}X&#8217;)) = (n - p)\sigma^{2}\qquad(12)$$
将（12）带入（7），即可得到$$$E(\widehat{\sigma}^2|X) = \sigma^{2}$$$</p></blockquote>

<p>所以，综上所述我们最终得到$$$\widehat{\beta}$$$的误差为:$$\widehat{cov}(\widehat{\beta}|X) = \widehat{\sigma}^{2}(X&#8217;X)^{-1}\qquad(13)$$</p>

<p>接下来我们需要考虑的问题是，我们的模型到底拟合的好不好。可以证明，假定方程有截距，那么$$$var(Y) = var(X\widehat{\beta}) + var(e)$$$，证明过程如下：</p>

<blockquote><p>令$$$u = I_{n * 1}$$$, 则$$$Y - \bar{Y}u = X\widehat{\beta} - \bar{Y}u + e$$$, 又因为e正交于X和u，那么，
$$||Y - \bar{Y}u||^{2} = ||X\widehat{\beta} - \bar{Y}u||^{2} + ||e||^{2}\qquad(14)$$
又因为$$$||X\widehat{\beta} - \bar{Y}u||^{2} = nvar(Y)$$$, $$$||X\widehat{\beta} - \bar{Y}u||^{2} = ||X\widehat{\beta} - \bar{X}\widehat{\beta}u||^{2} = nvar(X\widehat{\beta})$$$, $$$||e||^{2} = nvar(e)$$$，代入(14)，命题得证。</p></blockquote>

<p>因此我们看出可以把Y的波动分为两部分，一部分和自变量相关，一部分和残差相关。显然和残差相关的波动越小，说明我们拟合的模型和观测值越接近，因此我们定义了:
$$R^{2} = var(X\widehat{\beta}) / var(Y)\qquad(15)$$
来度量模型的拟合度。根据$$$R^{2}$$$的定义式，我们也称其为被解释的方差。但是，需要注意的是，$$$R^{2}$$$只能说明模型拟合的程度，仅仅依据$$$R^{2}$$$我们无法得到任何因果相关的结论。</p>
]]></content>
  </entry>
  
</feed>
